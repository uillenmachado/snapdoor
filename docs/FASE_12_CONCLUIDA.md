# ğŸ“‹ FASE 12 CONCLUÃDA - Scraper AvanÃ§ado (Sistema de Queue)

> **Status**: âœ… Completo  
> **Data**: 2025-10-14  
> **Commit**: d0dc4c7  
> **Build Time**: 24.38s  
> **Bundle Size**: 2,733KB (+14KB desde FASE 11)

---

## ğŸ“Š VisÃ£o Geral

A FASE 12 implementa um **sistema robusto de queue para processamento de scraping LinkedIn**, com retry logic, rate limiting, logs detalhados e monitoramento em tempo real.

### ğŸ¯ Objetivos AlcanÃ§ados

- âœ… Sistema de queue com priorizaÃ§Ã£o
- âœ… Retry logic com exponential backoff
- âœ… Rate limiting configurÃ¡vel
- âœ… Logs detalhados (4 nÃ­veis)
- âœ… EstatÃ­sticas agregadas
- âœ… Monitoramento em tempo real
- âœ… Webhook support (estrutura preparada)
- âœ… Concurrency-safe (SKIP LOCKED)

---

## ğŸ—ï¸ Arquitetura do Sistema

### Diagrama de Fluxo

```
User Action
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Enqueue Job    â”‚ â”€â”€â–º Rate Limit Check
â”‚  (Frontend)     â”‚     (10 jobs/min)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  scraper_queue     â”‚
â”‚  status: pending   â”‚
â”‚  priority: 0-10    â”‚
â”‚  scheduled_at: NOW â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Queue Processor     â”‚ â—„â”€â”€ SKIP LOCKED
â”‚  (Edge Function)     â”‚     (Concurrency)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â–º ğŸŸ¢ Success â”€â”€â–º status: completed
         â”‚                   result: {...}
         â”‚                   webhook: âœ“
         â”‚
         â””â”€â”€â–º ğŸ”´ Error â”€â”€â”€â–º status: failed
                            attempts++
                            retry with backoff
                            (2^n minutes)

scraper_logs Table
  â”œâ”€â”€ debug logs
  â”œâ”€â”€ info logs
  â”œâ”€â”€ warning logs
  â””â”€â”€ error logs

scraper_stats Table
  â””â”€â”€ Hourly aggregation
      â”œâ”€â”€ jobs_total
      â”œâ”€â”€ jobs_completed
      â”œâ”€â”€ jobs_failed
      â”œâ”€â”€ avg_duration
      â””â”€â”€ total_credits
```

---

## ğŸ—„ï¸ Estrutura de Banco de Dados

### 1. Tabela `scraper_queue`

Gerencia a fila de jobs de scraping.

```sql
CREATE TABLE scraper_queue (
  id UUID PRIMARY KEY,
  job_type TEXT CHECK (job_type IN ('profile', 'company', 'search', 'bulk')),
  priority INTEGER DEFAULT 0, -- Higher = first
  linkedin_url TEXT NOT NULL,
  lead_id UUID REFERENCES leads(id),
  company_id UUID REFERENCES companies(id),
  user_id UUID REFERENCES profiles(id) NOT NULL,
  team_id UUID REFERENCES teams(id),
  
  -- Status tracking
  status TEXT DEFAULT 'pending', -- pending/processing/completed/failed/cancelled/expired
  attempts INTEGER DEFAULT 0,
  max_attempts INTEGER DEFAULT 3,
  last_error TEXT,
  result JSONB,
  
  -- Timing
  scheduled_at TIMESTAMPTZ DEFAULT NOW(),
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  
  -- Webhooks
  webhook_url TEXT,
  webhook_delivered BOOLEAN DEFAULT false,
  webhook_delivered_at TIMESTAMPTZ,
  
  metadata JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Status Flow**:
```
pending â”€â”€â–º processing â”€â”€â–º completed âœ“
                   â”‚
                   â””â”€â”€â–º failed â”€â”€â–º pending (retry)
                           â”‚
                           â””â”€â”€â–º failed (max attempts)
```

**Indexes** (9 total):
- `idx_scraper_queue_status`
- `idx_scraper_queue_user_id`
- `idx_scraper_queue_team_id`
- `idx_scraper_queue_scheduled_at`
- `idx_scraper_queue_priority`
- `idx_scraper_queue_lead_id`
- `idx_scraper_queue_company_id`
- `idx_scraper_queue_processing` (composite: status + priority + scheduled_at)

---

### 2. Tabela `scraper_logs`

Logs detalhados para cada job.

```sql
CREATE TABLE scraper_logs (
  id UUID PRIMARY KEY,
  job_id UUID REFERENCES scraper_queue(id) ON DELETE CASCADE NOT NULL,
  level TEXT CHECK (level IN ('debug', 'info', 'warning', 'error')),
  message TEXT NOT NULL,
  details JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**NÃ­veis de Log**:
- `debug`: InformaÃ§Ãµes tÃ©cnicas detalhadas
- `info`: InformaÃ§Ãµes gerais (ex: "Starting scrape...")
- `warning`: Avisos nÃ£o-crÃ­ticos (ex: "Rate limit approaching...")
- `error`: Erros que causam falha

**Indexes**:
- `idx_scraper_logs_job_id`
- `idx_scraper_logs_level`
- `idx_scraper_logs_created_at`

---

### 3. Tabela `scraper_stats`

EstatÃ­sticas agregadas por hora/dia/usuÃ¡rio.

```sql
CREATE TABLE scraper_stats (
  id UUID PRIMARY KEY,
  date DATE NOT NULL,
  hour INTEGER CHECK (hour >= 0 AND hour < 24),
  user_id UUID,
  team_id UUID,
  
  -- Metrics
  jobs_total INTEGER DEFAULT 0,
  jobs_completed INTEGER DEFAULT 0,
  jobs_failed INTEGER DEFAULT 0,
  jobs_cancelled INTEGER DEFAULT 0,
  avg_duration_seconds NUMERIC(10, 2),
  total_credits_used INTEGER DEFAULT 0,
  rate_limit_hits INTEGER DEFAULT 0,
  
  UNIQUE(date, hour, user_id, team_id)
);
```

**AgregaÃ§Ã£o**: Trigger `trigger_update_scraper_stats` atualiza automaticamente ao completar/falhar job.

---

## ğŸ”§ FunÃ§Ãµes PostgreSQL

### 1. `get_next_scraper_job()`

Retorna prÃ³ximo job da fila (concurrency-safe).

```sql
SELECT * FROM get_next_scraper_job();
```

**LÃ³gica**:
- Filtra `status = 'pending'`
- Ordena por `priority DESC, scheduled_at ASC`
- `FOR UPDATE SKIP LOCKED` (evita race conditions)
- Retorna 1 job por vez

---

### 2. `mark_job_processing(job_id)`

Marca job como em processamento.

```sql
SELECT mark_job_processing('uuid-here');
```

**Efeito**:
- `status = 'processing'`
- `started_at = NOW()`

---

### 3. `mark_job_completed(job_id, result_jsonb)`

Marca job como completo.

```sql
SELECT mark_job_completed('uuid-here', '{"name": "John Doe", ...}'::jsonb);
```

**Efeito**:
- `status = 'completed'`
- `result = {...}`
- `completed_at = NOW()`

---

### 4. `mark_job_failed(job_id, error_text)`

Marca job como falhado com retry logic.

```sql
SELECT mark_job_failed('uuid-here', 'Connection timeout');
```

**LÃ³gica**:
```typescript
attempts++;
if (attempts >= max_attempts) {
  status = 'failed'; // Terminal state
} else {
  status = 'pending'; // Retry
  scheduled_at = NOW() + 2^attempts minutes; // Exponential backoff
}
```

**Exemplo de Backoff**:
- Tentativa 1 falha â†’ retry em 2 min
- Tentativa 2 falha â†’ retry em 4 min
- Tentativa 3 falha â†’ retry em 8 min
- Tentativa 4+ â†’ status = 'failed' (terminal)

---

### 5. `add_scraper_log(job_id, level, message, details)`

Adiciona log entry.

```sql
SELECT add_scraper_log(
  'job-uuid',
  'info',
  'Profile scraped successfully',
  '{"profile_id": 123}'::jsonb
);
```

---

### 6. `update_scraper_stats(user_id, team_id, status, duration, credits)`

Atualiza estatÃ­sticas (chamado por trigger).

```sql
SELECT update_scraper_stats(
  'user-uuid',
  'team-uuid',
  'completed',
  12.5, -- duration_seconds
  1     -- credits_used
);
```

**LÃ³gica**: UPSERT com agregaÃ§Ã£o incremental de mÃ©tricas.

---

### 7. `check_rate_limit(user_id, limit, window_minutes)`

Verifica se usuÃ¡rio estÃ¡ dentro do rate limit.

```sql
SELECT check_rate_limit(
  'user-uuid',
  10,  -- limit: 10 jobs
  1    -- window: 1 minute
);
-- Returns: true/false
```

**Uso**: Chamado antes de enqueue job no frontend.

---

## ğŸ£ Hooks React Query

### Hook `useScraperQueue.ts` (450 linhas)

#### Queries (6)

**1. useScraperJobs(status?, limit?)**
```typescript
const { data: jobs } = useScraperJobs('pending', 50);
// Auto-refetch: 10s
```

**2. useScraperJob(jobId)**
```typescript
const { data: job } = useScraperJob('uuid-here');
// Auto-refetch: 5s (para job ativo)
```

**3. useScraperLogs(jobId)**
```typescript
const { data: logs } = useScraperLogs('uuid-here');
// Auto-refetch: 5s (logs em tempo real)
```

**4. useScraperStats(days?)**
```typescript
const { data: stats } = useScraperStats(7);
// Ãšltimos 7 dias
```

**5. useQueueDashboard()**
```typescript
const { data: dashboard } = useQueueDashboard();
// Auto-refetch: 30s
// Returns: { pending_jobs, processing_jobs, completed_jobs, failed_jobs, cancelled_jobs, avg_duration_seconds }
```

**6. useRecentJobs()**
```typescript
const { data: recent } = useRecentJobs();
// Ãšltimos 100 jobs com JOIN de leads/companies
```

---

#### Mutations (6)

**1. useEnqueueJob()**
```typescript
const enqueue = useEnqueueJob();

enqueue.mutate({
  job_type: 'profile',
  linkedin_url: 'https://linkedin.com/in/johndoe',
  lead_id: 'lead-uuid',
  priority: 5,
  webhook_url: 'https://myapp.com/webhook'
});
```

**Rate Limit**: Verifica automaticamente antes de enqueue (10 jobs/min).

---

**2. useRetryJob()**
```typescript
const retry = useRetryJob();

retry.mutate('job-uuid');
// Reseta attempts para 0, status para 'pending'
```

---

**3. useCancelJob()**
```typescript
const cancel = useCancelJob();

cancel.mutate('job-uuid');
// Apenas se status = 'pending' ou 'processing'
```

---

**4. useDeleteJob()**
```typescript
const deleteJob = useDeleteJob();

deleteJob.mutate('job-uuid');
// Apenas se status = 'completed', 'failed', 'cancelled'
```

---

**5. useBulkEnqueueJobs()**
```typescript
const bulkEnqueue = useBulkEnqueueJobs();

bulkEnqueue.mutate([
  { job_type: 'profile', linkedin_url: '...' },
  { job_type: 'profile', linkedin_url: '...' },
  // ... atÃ© 50 jobs
]);
```

**Rate Limit**: 50 jobs em janela de 5 minutos.

---

**6. useClearJobs(status)**
```typescript
const clear = useClearJobs();

clear.mutate('completed'); // Remove todos os jobs completed
```

---

## ğŸ“± PÃ¡gina ScraperLogs (500 linhas)

### Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper Logs                       [Atualizar]        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Pendentesâ”‚ â”‚Processingâ”‚ â”‚Taxa de   â”‚ â”‚Falhados  â”‚ â”‚
â”‚  â”‚    12    â”‚ â”‚    3     â”‚ â”‚Sucesso   â”‚ â”‚    2     â”‚ â”‚
â”‚  â”‚          â”‚ â”‚          â”‚ â”‚   94%    â”‚ â”‚          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚Total Jobsâ”‚ â”‚Tempo     â”‚ â”‚CrÃ©ditos  â”‚              â”‚
â”‚  â”‚   150    â”‚ â”‚MÃ©dio 8s  â”‚ â”‚Usados    â”‚              â”‚
â”‚  â”‚          â”‚ â”‚          â”‚ â”‚   150    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Fila de Jobs           â”‚  Logs Detalhados             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ [All|Pending|...] â”‚  â”‚  â”‚ Selecione um job       â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚  â”‚ para ver logs          â”‚ â”‚
â”‚  â”‚ Job #123          â”‚â—„â”€â”¼â”€â”€â”¤                        â”‚ â”‚
â”‚  â”‚ â”œâ”€ profile        â”‚  â”‚  â”‚ [INFO] Starting...     â”‚ â”‚
â”‚  â”‚ â”œâ”€ pending        â”‚  â”‚  â”‚ [DEBUG] HTTP GET...    â”‚ â”‚
â”‚  â”‚ â””â”€ 2min ago       â”‚  â”‚  â”‚ [INFO] Profile found   â”‚ â”‚
â”‚  â”‚ [Retry][Cancel]   â”‚  â”‚  â”‚ [ERROR] Timeout        â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ Job #124          â”‚  â”‚                              â”‚
â”‚  â”‚ ...               â”‚  â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Features

**7 MÃ©tricas Cards**:
1. Jobs Pendentes
2. Em Processamento
3. Taxa de Sucesso
4. Jobs Falhados
5. Total de Jobs (7 dias)
6. Tempo MÃ©dio
7. CrÃ©ditos Usados

**5 Tabs**:
- Todos
- Pendentes (com badge de count)
- Processando (com badge de count)
- Completos
- Falhados

**Tabela de Jobs**:
- Nome (Lead ou Company)
- Badge de job_type
- Tentativas (X/Y)
- Status badge (colorido)
- Progress bar (se processing)
- Data criaÃ§Ã£o (relative time)
- AÃ§Ãµes: Retry, Cancel, Delete

**Logs Viewer**:
- Badge de nÃ­vel (debug/info/warning/error)
- Timestamp relativo
- Mensagem do log
- JSON details (se disponÃ­vel)
- Auto-scroll para novo log

**Auto-refresh**:
- Jobs: 10s
- Logs: 5s
- Dashboard: 30s

---

## ğŸ”’ RLS Policies

### scraper_queue

```sql
-- SELECT: UsuÃ¡rios veem seus prÃ³prios jobs
CREATE POLICY "Users can view own jobs" ON scraper_queue
  FOR SELECT USING (user_id = auth.uid());

-- INSERT: UsuÃ¡rios criam jobs para si
CREATE POLICY "Users can insert own jobs" ON scraper_queue
  FOR INSERT WITH CHECK (user_id = auth.uid());

-- UPDATE: UsuÃ¡rios atualizam seus jobs
CREATE POLICY "Users can update own jobs" ON scraper_queue
  FOR UPDATE USING (user_id = auth.uid());

-- DELETE: UsuÃ¡rios deletam seus jobs
CREATE POLICY "Users can delete own jobs" ON scraper_queue
  FOR DELETE USING (user_id = auth.uid());
```

### scraper_logs

```sql
-- SELECT: UsuÃ¡rios veem logs dos seus jobs
CREATE POLICY "Users can view logs for their jobs" ON scraper_logs
  FOR SELECT USING (
    job_id IN (SELECT id FROM scraper_queue WHERE user_id = auth.uid())
  );
```

**Nota**: InserÃ§Ã£o de logs Ã© feita por Edge Function (service_role), nÃ£o por usuÃ¡rios.

### scraper_stats

```sql
-- SELECT: UsuÃ¡rios veem suas prÃ³prias stats
CREATE POLICY "Users can view own stats" ON scraper_stats
  FOR SELECT USING (user_id = auth.uid());
```

---

## âš¡ Rate Limiting

### ConfiguraÃ§Ã£o Atual

**Jobs individuais**: 10 jobs / 1 minuto  
**Jobs bulk**: 50 jobs / 5 minutos

### Como Funciona

```typescript
// Frontend check antes de enqueue
const { data: canEnqueue } = await supabase.rpc('check_rate_limit', {
  p_user_id: userId,
  p_limit: 10,
  p_window_minutes: 1
});

if (!canEnqueue) {
  throw new Error('Rate limit exceeded');
}
```

### CustomizaÃ§Ã£o

Altere na funÃ§Ã£o `check_rate_limit`:

```sql
CREATE OR REPLACE FUNCTION check_rate_limit(
  p_user_id UUID,
  p_limit INTEGER DEFAULT 20, -- Aumentar para 20 jobs/min
  p_window_minutes INTEGER DEFAULT 1
)
```

---

## ğŸ” Retry Logic

### Exponential Backoff

```
Attempt 1 fails â†’ Retry em 2 min (2^1)
Attempt 2 fails â†’ Retry em 4 min (2^2)
Attempt 3 fails â†’ Retry em 8 min (2^3)
Attempt 4 â†’ Status = 'failed' (terminal)
```

### ConfiguraÃ§Ã£o

**Max Attempts**: 3 (padrÃ£o), configurÃ¡vel por job:

```typescript
enqueue.mutate({
  job_type: 'profile',
  linkedin_url: '...',
  max_attempts: 5 // Override padrÃ£o
});
```

### Manual Retry

UsuÃ¡rio pode clicar "Retry" para:
- Resetar `attempts = 0`
- Mudar `status = 'pending'`
- Limpar `last_error`
- Processar imediatamente (`scheduled_at = NOW()`)

---

## ğŸ“¡ Webhook Support

### Estrutura Preparada

Jobs podem ter `webhook_url` que serÃ¡ chamado ao completar.

```typescript
enqueue.mutate({
  job_type: 'profile',
  linkedin_url: '...',
  webhook_url: 'https://myapp.com/webhook/scraper'
});
```

### Payload (quando implementado)

```json
POST https://myapp.com/webhook/scraper
{
  "event": "job.completed",
  "job_id": "uuid-here",
  "job_type": "profile",
  "status": "completed",
  "result": {
    "name": "John Doe",
    "title": "CEO",
    "company": "Acme Inc"
  },
  "completed_at": "2025-10-14T10:30:00Z",
  "duration_seconds": 12.5
}
```

**Headers**:
```
X-SnapDoor-Signature: sha256=...
X-SnapDoor-Event: job.completed
```

**TODO**: Implementar Edge Function para delivery.

---

## ğŸ§ª Como Testar

### 1. Enqueue Job Manual

```typescript
// Em qualquer componente
const enqueue = useEnqueueJob();

enqueue.mutate({
  job_type: 'profile',
  linkedin_url: 'https://linkedin.com/in/test',
  lead_id: 'existing-lead-uuid',
  priority: 5
});
```

### 2. Ver Job na Queue

1. Ir para `/scraper-logs`
2. Tab "Pendentes"
3. Ver job listado

### 3. Simular Processamento (SQL)

```sql
-- Marcar como processing
SELECT mark_job_processing('job-uuid');

-- Adicionar logs
SELECT add_scraper_log('job-uuid', 'info', 'Starting scrape...');
SELECT add_scraper_log('job-uuid', 'debug', 'HTTP GET linkedin.com');

-- Marcar como completed
SELECT mark_job_completed('job-uuid', '{"name": "Test"}'::jsonb);
```

### 4. Simular Falha com Retry

```sql
SELECT mark_job_failed('job-uuid', 'Connection timeout');
-- Checa: attempts = 1, scheduled_at = NOW() + 2 min
```

### 5. Testar Rate Limit

```typescript
// Criar 11 jobs rapidamente
for (let i = 0; i < 11; i++) {
  enqueue.mutate({ ... });
}
// 11Âº job deve falhar com "Rate limit exceeded"
```

---

## ğŸ“Š EstatÃ­sticas e Monitoramento

### Views DisponÃ­veis

**1. scraper_queue_dashboard**
```sql
SELECT * FROM scraper_queue_dashboard WHERE user_id = auth.uid();
```

Retorna:
- pending_jobs
- processing_jobs
- completed_jobs
- failed_jobs
- cancelled_jobs
- avg_duration_seconds
- last_job_at

**2. scraper_recent_jobs**
```sql
SELECT * FROM scraper_recent_jobs LIMIT 100;
```

Retorna: Ãšltimos 100 jobs com JOIN de leads e companies.

---

## ğŸš§ LimitaÃ§Ãµes Conhecidas

### 1. Edge Function NÃ£o Implementada

**Problema**: Jobs nÃ£o sÃ£o processados automaticamente.

**Workaround**: Processar manualmente via SQL (para teste).

**SoluÃ§Ã£o Futura**: Criar `supabase/functions/process-scraper-queue/index.ts` com:
- Cron job (invoke a cada 10s)
- Pega job com `get_next_scraper_job()`
- Faz scraping real (LinkedIn API ou Puppeteer)
- Marca como completed/failed

---

### 2. Webhooks NÃ£o Entregues

**Problema**: `webhook_url` existe mas nÃ£o Ã© chamado.

**SoluÃ§Ã£o Futura**: Edge Function chama webhook ao completar:
```typescript
if (job.webhook_url) {
  await fetch(job.webhook_url, {
    method: 'POST',
    headers: {
      'X-SnapDoor-Signature': generateSignature(payload),
      'X-SnapDoor-Event': 'job.completed'
    },
    body: JSON.stringify(payload)
  });
  
  await supabase
    .from('scraper_queue')
    .update({ webhook_delivered: true, webhook_delivered_at: new Date() })
    .eq('id', jobId);
}
```

---

### 3. Sem Scraping Real

**Problema**: Sistema sÃ³ gerencia queue, nÃ£o faz scraping.

**IntegraÃ§Ã£o NecessÃ¡ria**:
- Biblioteca: `puppeteer` ou `playwright`
- Proxy rotation (evitar block)
- CAPTCHA solving (2captcha, anticaptcha)
- LinkedIn auth tokens

---

### 4. Sem PriorizaÃ§Ã£o AutomÃ¡tica

**Problema**: Todos os jobs tÃªm `priority = 0` por padrÃ£o.

**SoluÃ§Ã£o Futura**: LÃ³gica de priorizaÃ§Ã£o:
- Jobs de leads "quentes" â†’ priority = 10
- Jobs de bulk â†’ priority = 1
- Jobs manuais â†’ priority = 5

---

## ğŸ”® Melhorias Futuras

### 1. Edge Function Completa

**Arquivo**: `supabase/functions/process-scraper-queue/index.ts`

**Funcionalidades**:
- Invoke via Cron (a cada 10-30s)
- Pega prÃ³ximo job com `get_next_scraper_job()`
- Executa scraping (Puppeteer + Proxy)
- Logs detalhados em tempo real
- Retry automÃ¡tico com backoff
- Webhook delivery
- Rate limiting do LinkedIn (1 req/2s)

---

### 2. Scraping Services

**Profile Scraper**:
```typescript
async function scrapeLinkedInProfile(url: string) {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  
  await page.goto(url);
  
  const data = await page.evaluate(() => ({
    name: document.querySelector('.profile-name').textContent,
    title: document.querySelector('.profile-title').textContent,
    company: document.querySelector('.company-name').textContent,
    // ...
  }));
  
  await browser.close();
  return data;
}
```

---

### 3. Webhook Signature Verification

**Server-side**:
```typescript
import crypto from 'crypto';

function verifyWebhookSignature(payload: string, signature: string, secret: string) {
  const hash = crypto
    .createHmac('sha256', secret)
    .update(payload)
    .digest('hex');
  
  return `sha256=${hash}` === signature;
}
```

---

### 4. Priority Queue Algorithm

**Auto-priorizaÃ§Ã£o**:
```typescript
function calculatePriority(lead: Lead, job_type: string): number {
  let priority = 5; // Base

  if (lead.status === 'hot') priority += 3;
  if (lead.last_contact_days > 30) priority += 2;
  if (job_type === 'bulk') priority -= 3;
  
  return Math.max(0, Math.min(10, priority)); // 0-10 range
}
```

---

### 5. Job Dependencies

**Tabela adicional**:
```sql
CREATE TABLE job_dependencies (
  job_id UUID REFERENCES scraper_queue(id),
  depends_on UUID REFERENCES scraper_queue(id),
  PRIMARY KEY (job_id, depends_on)
);
```

**LÃ³gica**: Job sÃ³ processa se `depends_on` estÃ¡ completed.

---

### 6. Job Pause/Resume

**Campos**:
```sql
ALTER TABLE scraper_queue 
ADD COLUMN paused BOOLEAN DEFAULT false,
ADD COLUMN paused_at TIMESTAMPTZ,
ADD COLUMN paused_by UUID REFERENCES profiles(id);
```

**UI**: BotÃ£o "Pause" na tabela de jobs.

---

### 7. Bulk Import via CSV

**Componente**: `BulkEnqueueDialog.tsx`

**Funcionalidade**:
- Upload CSV com colunas: `linkedin_url`, `priority`, `lead_id`
- Parse e validaÃ§Ã£o
- Enqueue atÃ© 1000 jobs de uma vez
- Progress bar

---

### 8. Real-time Notifications

**Supabase Realtime**:
```typescript
const channel = supabase
  .channel('scraper-updates')
  .on('postgres_changes', {
    event: 'UPDATE',
    schema: 'public',
    table: 'scraper_queue',
    filter: `user_id=eq.${userId}`
  }, (payload) => {
    if (payload.new.status === 'completed') {
      toast.success(`Job ${payload.new.id} completado!`);
    }
  })
  .subscribe();
```

**Toast** ao completar job em background.

---

## ğŸ“ˆ MÃ©tricas de Sucesso

### Build Performance
- **Build time**: 24.38s
- **Modules**: 4,090 (+2 desde FASE 11)
- **Bundle size**: 2,733KB (+14KB)

### Code Metrics
- **Arquivos novos**: 3 (migration, hook, page)
- **Arquivos modificados**: 2 (App.tsx, AppSidebar.tsx)
- **Linhas de cÃ³digo**: ~1,600 novas
- **Migration**: 620 linhas SQL

### Database Objects Created
- **Tables**: 3 (scraper_queue, scraper_logs, scraper_stats)
- **Functions**: 7 (get_next_job, mark_*, add_log, update_stats, check_rate_limit)
- **Triggers**: 3 (updated_at, team_id, stats)
- **Views**: 2 (dashboard, recent_jobs)
- **RLS Policies**: 10 (queue: 4, logs: 1, stats: 1)
- **Indexes**: 15+ (performance otimizado)

### React Query Hooks
- **Queries**: 6 (jobs, job, logs, stats, dashboard, recent)
- **Mutations**: 6 (enqueue, retry, cancel, delete, bulk, clear)
- **Auto-refetch**: 5s - 30s (dependendo da query)

---

## âœ… Checklist de ConclusÃ£o

- [x] Migration `20251013000009_scraper_queue.sql` criada (620 linhas)
- [x] 3 tabelas criadas (scraper_queue, scraper_logs, scraper_stats)
- [x] 7 funÃ§Ãµes PL/pgSQL implementadas
- [x] 3 triggers criados
- [x] 2 views de monitoring
- [x] 15+ indexes para performance
- [x] RLS policies em todas as tabelas
- [x] Hook `useScraperQueue.ts` com 12 funÃ§Ãµes (450 linhas)
- [x] PÃ¡gina `ScraperLogs.tsx` com dashboard (500 linhas)
- [x] Rota `/scraper-logs` adicionada
- [x] Menu item "Scraper Logs" adicionado
- [x] Rate limiting implementado (10 jobs/min)
- [x] Retry logic com exponential backoff
- [x] Auto-refetch em tempo real
- [x] Build bem-sucedido (24.38s)
- [x] Commit realizado (d0dc4c7)
- [x] Push para GitHub completo
- [x] DocumentaÃ§Ã£o criada (FASE_12_CONCLUIDA.md)

**Pendente para produÃ§Ã£o**:
- [ ] Edge Function `process-scraper-queue` (processador real)
- [ ] IntegraÃ§Ã£o com LinkedIn scraping (Puppeteer/Playwright)
- [ ] Webhook delivery implementation
- [ ] Proxy rotation para evitar blocks
- [ ] CAPTCHA solving integration

---

## ğŸ‰ Resumo

A **FASE 12** implementa a infraestrutura completa de queue para scraping:

âœ… **Queue System**: PriorizaÃ§Ã£o, concurrency-safe, SKIP LOCKED  
âœ… **Retry Logic**: Exponential backoff (2^n minutes)  
âœ… **Rate Limiting**: 10 jobs/min configurÃ¡vel  
âœ… **Logs Detalhados**: 4 nÃ­veis (debug/info/warning/error)  
âœ… **EstatÃ­sticas**: AgregaÃ§Ã£o automÃ¡tica por hora/dia/usuÃ¡rio  
âœ… **Monitoramento**: Dashboard em tempo real com auto-refresh  
âœ… **Webhooks**: Estrutura preparada (delivery pendente)  
âœ… **RLS**: Isolamento completo por usuÃ¡rio  

**PrÃ³xima Fase**: FASE 13 - OtimizaÃ§Ã£o & Performance (Lazy loading, code splitting, caching)

---

**DocumentaÃ§Ã£o criada por**: AI Assistant  
**Data**: 2025-10-14  
**VersÃ£o**: 1.0
